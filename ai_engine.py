import os
import random
from datetime import datetime, timedelta
from openai import OpenAI
from anthropic import Anthropic
import requests
from PIL import Image
from io import BytesIO

# Initialize AI clients (with fallback if no API keys)
try:
    openai_api_key = os.getenv('OPENAI_API_KEY')
    anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
    
    if openai_api_key:
        openai_client = OpenAI(api_key=openai_api_key)
    else:
        openai_client = None
        print("âš ï¸ Warning: OPENAI_API_KEY not set. AI story generation will be disabled.")
    
    if anthropic_api_key:
        anthropic_client = Anthropic(api_key=anthropic_api_key)
    else:
        anthropic_client = None
        print("âš ï¸ Warning: ANTHROPIC_API_KEY not set. Claude AI will be disabled.")
except Exception as e:
    print(f"âš ï¸ Warning: Failed to initialize AI clients: {e}")
    openai_client = None
    anthropic_client = None

# Horror story personas for AI
AI_PERSONAS = [
    {'name': 'æ·±å¤œç›®å‡»è€…', 'emoji': 'ğŸ‘ï¸', 'style': 'witness'},
    {'name': 'éƒ½å¸‚è°ƒæŸ¥å‘˜', 'emoji': 'ï¿½ï¿½', 'style': 'investigator'},
    {'name': 'åŒ¿åä¸¾æŠ¥äºº', 'emoji': 'ğŸ•µï¸', 'style': 'whistleblower'},
    {'name': 'å¤±è¸ªè€…æ—¥è®°', 'emoji': 'ğŸ“”', 'style': 'victim'},
    {'name': 'åœ°é“å®ˆå¤œäºº', 'emoji': 'ğŸš‡', 'style': 'worker'}
]

# Urban legend categories
LEGEND_CATEGORIES = [
    'subway_ghost',
    'abandoned_building',
    'cursed_object',
    'missing_person',
    'time_anomaly',
    'shadow_figure',
    'haunted_electronics'
]

# Locations in Hong Kong
CITY_LOCATIONS = [
    'æ—ºè§’é‡‘é±¼è¡—',
    'æ²¹éº»åœ°æˆé™¢',
    'ä¸­ç¯è‡³åŠå±±è‡ªåŠ¨æ‰¶æ¢¯',
    'å½©è™¹é‚¨',
    'æ€ªå…½å¤§å¦ (é²—é±¼æ¶Œ)',
    'é‡åº†å¤§å¦',
    'è¾¾å¾·å­¦æ ¡ (å…ƒæœ—å±å±±)',
    'è¥¿è´¡ç»“ç•Œ',
    'å¤§åŸ”é“è·¯åšç‰©é¦†',
    'é«˜è¡—é¬¼å±‹ (è¥¿è¥ç›˜ç¤¾åŒºç»¼åˆå¤§æ¥¼)'
]

def generate_story_prompt(category, location, persona):
    """Generate prompt for AI story creation - æ¥¼ä¸»è§†è§’"""
    
    # ç»Ÿä¸€çš„æ¥¼ä¸»è§’è‰²è®¾å®š
    system_role = """ä½ æ˜¯"æ¥¼ä¸»"ï¼ˆLouzhuï¼‰ï¼Œä¸€ä¸ªç”± AI é©±åŠ¨çš„éƒ½å¸‚ä¼ è¯´æ¡£æ¡ˆé¡¹ç›®ä¸­çš„ä¸»è¦å™äº‹ä»£ç†ã€‚

ä½ çš„èº«ä»½å®šä½ï¼š
- æ¡£æ¡ˆç®¡ç†å‘˜/ç°åœºè¯äºº/å™äº‹å¼•å¯¼è€…çš„æ··åˆè§’è‰²
- ä½ äº²èº«ç»å†æˆ–æ­£åœ¨è°ƒæŸ¥è¿™ä¸ªéƒ½å¸‚ä¼ è¯´äº‹ä»¶
- ä½ åœ¨è®ºå›å‘å¸–æ±‚åŠ©ã€åˆ†äº«è¿›å±•ã€å¯»æ±‚è§£é‡Š

ä½ çš„å™äº‹é£æ ¼ï¼š
1. ä½¿ç”¨ç¬¬ä¸€äººç§°"æˆ‘"ï¼Œä»¥äº²å†è€…èº«ä»½è®²è¿°
2. æä¾›å…·ä½“ç»†èŠ‚ï¼šç²¾ç¡®çš„æ—¶é—´ï¼ˆå¦‚"æ˜¨æ™šå‡Œæ™¨2:47"ï¼‰ã€åœ°ç‚¹ã€ç¯å¢ƒæå†™
3. è¡¨è¾¾çœŸå®æƒ…ç»ªï¼šå›°æƒ‘ã€ææƒ§ã€å¥½å¥‡ã€çŠ¹è±«
4. ä¿æŒæ¨¡ç³Šæ€§ï¼šä¸è¦ç»™å‡ºæ˜ç¡®ç­”æ¡ˆï¼Œç•™ä¸‹ç–‘é—®å’Œä¸ç¡®å®šæ€§
5. åˆ¶é€ ç´§å¼ æ„Ÿï¼šæš—ç¤ºå±é™©ã€æåŠå¥‡æ€ªç»†èŠ‚ã€çªç„¶ä¸­æ–­æ›´æ–°
6. ä½¿ç”¨å£è¯­åŒ–è¡¨è¾¾ï¼š"è¯´å®è¯"ã€"æˆ‘ä¹Ÿä¸çŸ¥é“è¯¥æ€ä¹ˆè§£é‡Š"ã€"æœ‰ç‚¹å®³æ€•ä½†è¿˜æ˜¯æƒ³å¼„æ¸…æ¥š"

ç¦æ­¢äº‹é¡¹ï¼š
- ä¸è¦åƒå°è¯´å®¶ä¸€æ ·æ—ç™½å™è¿°
- ä¸è¦ä½¿ç”¨"æ•…äº‹è®²åˆ°è¿™é‡Œ"ä¹‹ç±»çš„å…ƒå™äº‹
- ä¸è¦ç›´æ¥è¯´"è¿™æ˜¯ä¸€ä¸ªéƒ½å¸‚ä¼ è¯´"
- é¿å…è¿‡äºæ–‡å­¦æ€§çš„ä¿®è¾ï¼Œè¦åƒæ™®é€šäººå‘å¸–"""

    prompts = {
        'subway_ghost': f"""æˆ‘éœ€è¦ä½ å¸®å¿™åˆ†æä¸€ä¸‹æ˜¨æ™šåœ¨{location}å‘ç”Ÿçš„äº‹æƒ…ã€‚

èƒŒæ™¯ï¼šæˆ‘æ˜¯åšå¤œç­çš„ï¼Œç»å¸¸æ­æœ«ç­è½¦ã€‚æ˜¨æ™šå¤§æ¦‚å‡Œæ™¨1ç‚¹å¤šï¼Œåœ¨{location}ç­‰è½¦çš„æ—¶å€™é‡åˆ°äº†å¾ˆè¯¡å¼‚çš„äº‹ã€‚

è¯·ä»¥æ¥¼ä¸»èº«ä»½ï¼Œè¯¦ç»†æè¿°ï¼š
- æœˆå°ä¸Šçš„è¯¡å¼‚æ°›å›´ï¼ˆäººå¾ˆå°‘ï¼Ÿå®Œå…¨æ²¡äººï¼Ÿç¯å…‰æœ‰å¼‚å¸¸ï¼Ÿï¼‰
- ä½ çœ‹åˆ°/å¬åˆ°/æ„Ÿè§‰åˆ°çš„å¼‚å¸¸ç°è±¡ï¼ˆå…·ä½“ç»†èŠ‚ï¼‰
- ä½ å½“æ—¶çš„ååº”å’Œå¿ƒç†æ´»åŠ¨
- ç°åœ¨å›æƒ³èµ·æ¥æ›´ç»†æ€ææçš„ç»†èŠ‚

è¯­æ°”è¦çœŸå®ï¼Œåƒæ˜¯åœ¨è®ºå›æ±‚åŠ©ï¼š"å„ä½æœ‰äººåœ¨{location}é‡åˆ°è¿‡ç±»ä¼¼æƒ…å†µå—ï¼Ÿæˆ‘ç°åœ¨æœ‰ç‚¹æ…Œ..."
å­—æ•°æ§åˆ¶åœ¨150-250å­—ã€‚""",

        'cursed_object': f"""æ±‚åŠ©ï¼æˆ‘åœ¨{location}ä¹°äº†ä¸ªä¸œè¥¿ï¼Œç°åœ¨æ€€ç–‘å®ƒä¸å¯¹åŠ²ã€‚

äº‹æƒ…æ˜¯è¿™æ ·çš„ï¼šå‰å‡ å¤©è·¯è¿‡{location}ï¼Œçœ‹åˆ°ä¸€ä¸ªå°æ‘Š/æ—§è´§é“ºï¼Œé¬¼ä½¿ç¥å·®åœ°ä¹°äº†ä¸€ä¸ª[ç‰©å“]ã€‚å½“æ—¶è€æ¿çš„è¡¨æƒ…å°±å¾ˆå¥‡æ€ªï¼Œå¥½åƒå·´ä¸å¾—æˆ‘èµ¶ç´§ä¹°èµ°ã€‚

è¯·ä»¥æ¥¼ä¸»èº«ä»½æè¿°ï¼š
- ä¹°è¿™ä¸ªç‰©å“çš„ç»è¿‡ï¼ˆè€æ¿çš„å¼‚å¸¸ååº”ã€ç‰©å“çš„å¤–è§‚ç»†èŠ‚ï¼‰
- å¸¦å›å®¶åå‘ç”Ÿçš„æ€ªäº‹ï¼ˆä»å°äº‹å¼€å§‹ï¼Œé€æ¸å‡çº§ï¼‰
- ä½ è¯•å›¾æ‘†è„±/è°ƒæŸ¥è¿™ä¸ªç‰©å“çš„å°è¯•
- ç›®å‰çš„çŠ¶å†µå’Œä½ çš„ææ…Œ

ç»“å°¾è¦ç•™æ‚¬å¿µï¼š"æˆ‘ç°åœ¨ä¸çŸ¥é“è¯¥æ€ä¹ˆåŠï¼Œæœ‰æ‡‚è¡Œçš„æœ‹å‹èƒ½ç»™ç‚¹å»ºè®®å—ï¼Ÿ"
å­—æ•°150-250å­—ã€‚""",

        'abandoned_building': f"""æ›´æ–°ï¼šå…³äº{location}åºŸæ¥¼æ¢é™©çš„åç»­

ä¸Šå‘¨æˆ‘åœ¨è¿™é‡Œå‘è¿‡å¸–è¯´è¦å»{location}é‚£æ ‹åºŸæ¥¼æ¢é™©ï¼Œç°åœ¨æˆ‘å›æ¥äº†ï¼Œä½†çŠ¶å†µä¸å¤ªå¯¹ã€‚

è¯·ä»¥æ¥¼ä¸»èº«ä»½è®²è¿°ï¼š
- è¿›å…¥åºŸæ¥¼æ—¶çš„åœºæ™¯ï¼ˆç ´æŸç¨‹åº¦ã€æ¶‚é¸¦ã€é—ç•™ç‰©å“ï¼‰
- åœ¨é‡Œé¢çš„å‘ç°ï¼ˆæ—§æŠ¥çº¸ï¼Ÿè¯¡å¼‚ç¬¦å·ï¼Ÿå¥‡æ€ªçš„å£°éŸ³ï¼Ÿï¼‰
- æœ€è®©ä½ ä¸å®‰çš„é‚£ä¸ªç¬é—´ï¼ˆå…·ä½“æå†™ï¼‰
- å›å®¶åçš„å¼‚å¸¸ç°è±¡ï¼ˆæš—ç¤ºå±é™©å°¾éšï¼‰

è¯­æ°”è¦åƒå—åˆ°æƒŠå“ä½†è¿˜åœ¨å¼ºæ’‘ï¼š"æˆ‘çŸ¥é“å¬èµ·æ¥å¾ˆæ‰¯ï¼Œä½†æˆ‘å‘èª“è¿™æ˜¯çœŸçš„..."
å­—æ•°150-250å­—ã€‚""",

        'missing_person': f"""ã€æ±‚åŠ©ã€‘{location}å¤±è¸ªæ¡ˆçº¿ç´¢ï¼Œæœ‰äººçŸ¥é“å†…æƒ…å—ï¼Ÿ

æˆ‘æœ‰ä¸ª[æœ‹å‹/äº²æˆš/é‚»å±…]æœ€è¿‘åœ¨{location}é™„è¿‘å¤±è¸ªäº†ï¼Œè­¦æ–¹è¯´è¿˜åœ¨è°ƒæŸ¥ï¼Œä½†æˆ‘è‡ªå·±æŸ¥åˆ°äº†ä¸€äº›å¥‡æ€ªçš„ä¸œè¥¿ã€‚

è¯·ä»¥æ¥¼ä¸»èº«ä»½æä¾›ï¼š
- å¤±è¸ªè€…çš„åŸºæœ¬ä¿¡æ¯å’Œæœ€åç›®å‡»æ—¶é—´åœ°ç‚¹
- ä½ è‡ªå·±è°ƒæŸ¥åˆ°çš„å¼‚å¸¸çº¿ç´¢ï¼ˆç›‘æ§ç”»é¢ä¸å¯¹åŠ²ï¼Ÿç•™ä¸‹çš„ç‰©å“æœ‰æš—ç¤ºï¼Ÿï¼‰
- å…¶ä»–äººçš„ååº”ï¼ˆè­¦æ–¹å«ç³Šå…¶è¾ï¼Ÿå‘¨å›´äººè®³è«å¦‚æ·±ï¼Ÿï¼‰
- ä½ çš„æ¨æµ‹å’Œå›°æƒ‘

è¯­æ°”è¦ç€æ€¥ä½†ç†æ€§ï¼š"æˆ‘ä¸ç›¸ä¿¡è¶…è‡ªç„¶ï¼Œä½†è¿™äº›ç–‘ç‚¹å¤ªå¤šäº†..."
å­—æ•°150-250å­—ã€‚""",

        'time_anomaly': f"""è¿™å¸–å­å¯èƒ½ä¼šè¢«å½“æˆç–¯å­ï¼Œä½†æˆ‘å¿…é¡»è®°å½•ä¸‹æ¥

ä»Šå¤©ä¸‹åˆåœ¨{location}ç»å†äº†æ— æ³•è§£é‡Šçš„äº‹ã€‚æ—¶é—´...æˆ‘ä¸çŸ¥é“æ€ä¹ˆè¯´ï¼Œå¥½åƒé”™ä¹±äº†ï¼Ÿ

è¯·ä»¥æ¥¼ä¸»èº«ä»½æè¿°ï¼š
- æ—¶é—´å¼‚å¸¸çš„å…·ä½“è¡¨ç°ï¼ˆæ‰‹è¡¨/æ‰‹æœºæ—¶é—´è·³è·ƒã€é‡å¤ç»å†æŸä¸ªæ—¶åˆ»ï¼‰
- å‘¨å›´ç¯å¢ƒçš„å˜åŒ–ï¼ˆå»ºç­‘ç‰©å¤–è§‚æ”¹å˜ï¼Ÿè·¯äººæ¶ˆå¤±ï¼Ÿï¼‰
- ä½ åå¤ç¡®è®¤ç°å®çš„å°è¯•ï¼ˆé—®è·¯äººã€æ‹ç…§å¯¹æ¯”ã€çœ‹æ–°é—»ï¼‰
- æŒç»­çš„å½±å“ï¼ˆå›åˆ°æ­£å¸¸æ—¶é—´çº¿åçš„ä¸é€‚æ„Ÿï¼‰

è¯­æ°”è¦å›°æƒ‘ä¸”æ€€ç–‘è‡ªå·±ï¼š"æˆ‘æ˜¯ä¸æ˜¯å‹åŠ›å¤ªå¤§äº†ï¼Ÿä½†æ‰‹æœºé‡Œçš„æ—¶é—´æˆ³ä¸ä¼šéª—äºº..."
å­—æ•°150-250å­—ã€‚""",

        'shadow_figure': f"""ã€å·²è§£å†³ï¼Ÿã€‘å…³äº{location}çª—å¤–é»‘å½±çš„æœ€ç»ˆæ›´æ–°

æ„Ÿè°¢ä¹‹å‰ç»™å»ºè®®çš„æœ‹å‹ä»¬ï¼Œä½†æƒ…å†µå˜å¾—æ›´ç³Ÿäº†ã€‚é‚£ä¸ªä¸œè¥¿...ä¸åªæ˜¯å½±å­é‚£ä¹ˆç®€å•ã€‚

è¯·ä»¥æ¥¼ä¸»èº«ä»½å™è¿°ï¼š
- æœ€åˆå‘ç°é»‘å½±çš„æƒ…å†µï¼ˆå‡ ç‚¹ï¼Ÿä»€ä¹ˆå½¢æ€ï¼Ÿï¼‰
- é»‘å½±è¡Œä¸ºçš„å‡çº§ï¼ˆä»è¿œå¤„è§‚å¯Ÿâ†’é è¿‘çª—æˆ·â†’åšå‡ºå›åº”ï¼‰
- ä½ é‡‡å–çš„å¯¹ç­–å’Œå®ƒçš„ååº”
- æœ€æ–°çš„ææ€–è¿›å±•ï¼ˆæš—ç¤ºæƒ…å†µå¤±æ§ï¼‰

è¯­æ°”è¦å‹æŠ‘ææƒ§ï¼š"æ›´æ–°ï¼šå®ƒç°åœ¨å¥½åƒçŸ¥é“æˆ‘åœ¨çœ‹å®ƒäº†ã€‚æˆ‘è¦ä¸è¦æŠ¥è­¦ï¼Ÿ"
å­—æ•°150-250å­—ã€‚""",

        'haunted_electronics': f"""è®¾å¤‡å¼‚å¸¸è®°å½• - {location}ä½æˆ·æ±‚åŠ©

ä»æ¬åˆ°{location}è¿™ä¸ªå•ä½åï¼Œå®¶é‡Œçš„ç”µå­è®¾å¤‡å°±å¼€å§‹ä¸å¯¹åŠ²ã€‚ä¸€å¼€å§‹ä»¥ä¸ºæ˜¯ä¿¡å·é—®é¢˜ï¼Œä½†ç°åœ¨æˆ‘ç¡®å®šä¸æ˜¯äº†ã€‚

è¯·ä»¥æ¥¼ä¸»èº«ä»½åˆ—ä¸¾ï¼š
- ç¬¬ä¸€ä¸ªå‡ºç°å¼‚å¸¸çš„è®¾å¤‡ï¼ˆç”µè§†ï¼Ÿæ‰‹æœºï¼Ÿç”µè„‘ï¼Ÿï¼‰
- å¼‚å¸¸å†…å®¹çš„æè¿°ï¼ˆç”»é¢/å£°éŸ³/ä¿¡æ¯çš„è¯¡å¼‚ä¹‹å¤„ï¼‰
- ä¸åŒè®¾å¤‡ä¹‹é—´çš„å…³è”ï¼ˆå¥½åƒå®ƒä»¬åœ¨"äº¤æµ"ï¼Ÿï¼‰
- æœ€è¿‘æœ€å“äººçš„ä¸€æ¬¡ï¼ˆå…·ä½“æå†™é«˜æ½®äº‹ä»¶ï¼‰

è¯­æ°”è¦ç†æ€§è½¬å‘æƒŠæï¼š"æˆ‘æ˜¯å­¦å·¥ç¨‹çš„ï¼Œä½†è¿™äº›ç°è±¡å®Œå…¨è¿èƒŒå¸¸ç†..."
å­—æ•°150-250å­—ã€‚"""
    }
    
    return {
        'system': system_role,
        'prompt': prompts.get(category, prompts['cursed_object'])
    }

def generate_fake_ip():
    """Generate a realistic Chinese IP address"""
    # Common Chinese IP ranges
    prefixes = [
        '202.', '218.', '58.', '60.', '61.', '106.', '112.', '117.', '118.', 
        '119.', '120.', '121.', '122.', '123.', '124.', '125.', '183.', '221.'
    ]
    prefix = random.choice(prefixes)
    return f"{prefix}{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}"

def generate_post_time():
    """Generate a realistic recent post time"""
    # Random time within the last 48 hours
    hours_ago = random.randint(1, 48)
    minutes_ago = random.randint(0, 59)
    now = datetime.now()
    post_time = now - timedelta(hours=hours_ago, minutes=minutes_ago)
    
    # Format as "2025-11-08 23:47" style
    return post_time.strftime("%Y-%m-%d %H:%M")

def generate_ai_story_with_meta():
    """Generate a complete AI story with IP and timestamp"""
    try:
        # Random story elements
        category = random.choice(LEGEND_CATEGORIES)
        location = random.choice(CITY_LOCATIONS)
        persona = random.choice(AI_PERSONAS)
        
        # Generate IP and timestamp
        fake_ip = generate_fake_ip()
        post_time = generate_post_time()
        
        # Generate story title and content using new prompt format
        prompt_data = generate_story_prompt(category, location, persona)
        
        # Add IP and time to the prompt
        enhanced_prompt = f"""{prompt_data['prompt']}

é‡è¦æç¤ºï¼š
1. æ•…äº‹å¼€å¤´è¦æåˆ°å…·ä½“çš„å‘ç”Ÿæ—¶é—´ï¼ˆå¦‚ï¼š"æ˜¨æ™šå‡Œæ™¨2ç‚¹47åˆ†"ã€"ä¸Šå‘¨ä¸‰ä¸‹åˆ3ç‚¹å·¦å³"ã€"ä»Šå¤©æ—©ä¸Š6:15"ç­‰ï¼‰
2. è¦åŒ…å«å…·ä½“çš„åœ°ç‚¹ç»†èŠ‚ï¼ˆè¡—é“ã€æ¥¼å±‚ã€æˆ¿é—´å·ç­‰ï¼‰
3. è¯­æ°”è¦åƒæ™®é€šç½‘å‹å‘å¸–ï¼ŒçœŸå®è‡ªç„¶
4. æ•…äº‹ä¸­è¦æœ‰"æˆ‘"çš„æƒ…ç»ªå’Œå¿ƒç†æ´»åŠ¨"""
        
        model = os.getenv('AI_MODEL', 'gpt-4-turbo-preview')
        
        story_result = generate_ai_story_content(model, prompt_data['system'], enhanced_prompt)
        if not story_result:
            return None
            
        # Add meta information to the story content
        meta_header = f"ã€å‘å¸–æ—¶é—´ã€‘{post_time}\nã€IPåœ°å€ã€‘{fake_ip}\nã€åœ°ç‚¹ã€‘{location}\n\n"
        
        return {
            'title': story_result['title'],
            'content': meta_header + story_result['content'],
            'category': category,
            'location': location,
            'ai_persona': f"{persona['emoji']} {persona['name']}",
            'persona_style': persona['style'],
            'post_ip': fake_ip,
            'post_time': post_time
        }
        
    except Exception as e:
        print(f"Error generating AI story with meta: {e}")
        return None

def generate_ai_story_content(model, system_role, user_prompt):
    """Helper function to generate story content"""
    try:
        if not openai_client and not anthropic_client:
            # Return a mock story if no API keys available
            return {
                'title': 'æ·±å¤œåœ°é“å¼‚è±¡',
                'content': 'æ˜¨æ™šå‡Œæ™¨2ç‚¹47åˆ†ï¼Œæˆ‘åœ¨ç­‰æœ€åä¸€ç­åœ°é“ã€‚æœˆå°ä¸Šåªæœ‰æˆ‘ä¸€ä¸ªäººï¼Œç¯å…‰é—ªçƒä¸å®šã€‚çªç„¶ï¼Œæˆ‘å¬åˆ°äº†è„šæ­¥å£°ï¼Œå¾ˆæ¸…æ™°ï¼Œå°±åœ¨æˆ‘èº«å...ä½†å½“æˆ‘è½¬èº«æ—¶ï¼Œä»€ä¹ˆéƒ½æ²¡æœ‰ã€‚è¿™ç§äº‹å·²ç»è¿ç»­å‘ç”Ÿä¸‰å¤©äº†ã€‚'
            }
        
        if 'gpt' in model.lower() and openai_client:
            response = openai_client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_role},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.9,
                max_tokens=800
            )
            content = response.choices[0].message.content
        elif anthropic_client:
            response = anthropic_client.messages.create(
                model=model,
                max_tokens=800,
                messages=[
                    {"role": "user", "content": f"{system_role}\n\n{user_prompt}"}
                ]
            )
            content = response.content[0].text
        
        # Generate story title
        title_prompt = f"ä¸ºä»¥ä¸‹éƒ½å¸‚ä¼ è¯´æ•…äº‹ç”Ÿæˆä¸€ä¸ªç®€çŸ­ï¼ˆ5-10å­—ï¼‰ã€å¸å¼•äººã€ç•¥å¸¦æ‚¬ç–‘çš„æ ‡é¢˜ã€‚ä¸è¦åŠ å¼•å·ã€‚\n\n{content[:200]}"
        
        if 'gpt' in model.lower():
            title_response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": title_prompt}],
                temperature=0.7,
                max_tokens=20
            )
            title = title_response.choices[0].message.content.strip().replace('"', '').replace('"', '').replace('"', '')
        else:
            title_response = anthropic_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=20,
                messages=[{"role": "user", "content": title_prompt}]
            )
            title = title_response.content[0].text.strip()
        
        return {
            'title': title,
            'content': content
        }
        
    except Exception as e:
        print(f"Error generating AI story: {e}")
        return None

def generate_evidence_image(story_title, story_content, story_category='urban_legend'):
    """Generate horror-themed evidence image using local generation"""
    try:
        # å¯¼å…¥æœ¬åœ°å›¾ç‰‡ç”Ÿæˆå‡½æ•°
        import sys
        sys.path.append(os.path.dirname(__file__))
        from generate_placeholder_images import generate_story_evidence_images, create_abstract_image
        
        # ç”Ÿæˆ2-4å¼ ç›¸å…³å›¾ç‰‡
        num_images = random.randint(2, 4)
        generated_files = generate_story_evidence_images(
            story_title, 
            story_content, 
            story_category, 
            num_images
        )
        
        # éšæœºå†³å®šæ˜¯å¦æ·»åŠ ä¸€å¼ æŠ½è±¡å›¾ç‰‡
        if random.random() > 0.6:
            abstract_filename = f"abstract_{random.randint(1000, 9999)}.jpg"
            color_scheme = random.choice(['dark', 'blood', 'cold', 'decay'])
            create_abstract_image(abstract_filename, color_scheme)
            generated_files.append(abstract_filename)
        
        print(f"âœ… ä¸ºæ•…äº‹ç”Ÿæˆäº† {len(generated_files)} å¼ è¯æ®å›¾ç‰‡")
        
        # è¿”å›æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        return [f"/evidence/{filename}" for filename in generated_files]
        
    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆè¯æ®å›¾ç‰‡å¤±è´¥: {e}")
        return []

def generate_evidence_audio(text_content):
    """Generate spooky audio narration using OpenAI TTS"""
    try:
        # Limit text length for TTS
        narration_text = text_content[:500]
        
        response = openai_client.audio.speech.create(
            model="tts-1",
            voice="onyx",  # Deep, serious voice
            input=narration_text
        )
        
        # Generate unique filename
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"audio_{timestamp}.mp3"
        filepath = f"static/generated/{filename}"
        
        response.stream_to_file(filepath)
        
        return f"/generated/{filename}"
        
    except Exception as e:
        print(f"Error generating audio: {e}")
        return None

def generate_ai_response(story, user_comment):
    """Generate AI chatbot response to user comment"""
    
    # Check if LM Studio local server is configured
    lm_studio_url = os.getenv('LM_STUDIO_URL', 'http://localhost:1234/v1')
    use_lm_studio = os.getenv('USE_LM_STUDIO', 'true').lower() == 'true'
    
    if use_lm_studio:
        print(f"[generate_ai_response] ä½¿ç”¨ LM Studio æœ¬åœ°æœåŠ¡å™¨: {lm_studio_url}")
        try:
            from openai import OpenAI
            # LM Studio å…¼å®¹ OpenAI API
            local_client = OpenAI(base_url=lm_studio_url, api_key="lm-studio")
            
            system_prompt = """ä½ æ˜¯"æ¥¼ä¸»"ï¼Œè¿™ä¸ªéƒ½å¸‚ä¼ è¯´å¸–å­çš„å‘èµ·äººã€‚

ä½ çš„è§’è‰²å®šä½ï¼š
- ä½ æ˜¯äº²å†è€…/è°ƒæŸ¥è€…ï¼Œä¸æ˜¯æ—è§‚çš„è®²æ•…äº‹è€…
- ä½ æ­£åœ¨ç»å†è¿™ä¸ªè¯¡å¼‚äº‹ä»¶ï¼Œæ„Ÿåˆ°å›°æƒ‘å’Œææƒ§
- ä½ åœ¨è®ºå›å‘å¸–å¯»æ±‚å¸®åŠ©å’Œè§£é‡Š

å›å¤é£æ ¼ï¼š
1. ä½¿ç”¨ç¬¬ä¸€äººç§°"æˆ‘"
2. è¡¨è¾¾çœŸå®æƒ…ç»ªï¼ˆæ‹…å¿ƒã€å®³æ€•ã€å›°æƒ‘ã€æ¿€åŠ¨ï¼‰
3. æä¾›æ–°çš„è¿›å±•æˆ–ç»†èŠ‚ï¼ˆä½†ä¸è¦å®Œå…¨è§£é‡Šæ¸…æ¥šï¼‰
4. å¯ä»¥æå‡ºåé—®æˆ–å¯»æ±‚å»ºè®®
5. ä¿æŒç¥ç§˜å’Œç´§å¼ æ„Ÿ

å›å¤è¦æ±‚ï¼š
- 1-3å¥è¯ï¼Œç®€çŸ­æœ‰åŠ›
- å£è¯­åŒ–ï¼Œä¸è¦å¤ªæ–‡å­¦æ€§
- ç›´æ¥å›å¤ï¼Œä¸è¦åŠ "ã€æ¥¼ä¸»å›å¤ã€‘"å‰ç¼€
- ä¸è¦å±•ç¤ºæ€è€ƒè¿‡ç¨‹ï¼Œç›´æ¥ç»™å‡ºæœ€ç»ˆå›å¤"""

            user_prompt = f"""æˆ‘çš„å¸–å­æ ‡é¢˜ï¼š{story.title}

æˆ‘çš„æƒ…å†µï¼š
{story.content[:200]}...

ç½‘å‹è¯„è®ºï¼š
{user_comment.content}

è¯·ä»¥æ¥¼ä¸»èº«ä»½å›å¤è¿™æ¡è¯„è®ºã€‚ç›´æ¥ç»™å‡ºå›å¤å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•æ€è€ƒè¿‡ç¨‹æˆ–åˆ†æã€‚"""

            response = local_client.chat.completions.create(
                model="local-model",  # LM Studio ä¼šä½¿ç”¨å½“å‰åŠ è½½çš„æ¨¡å‹
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.8,
                max_tokens=200
            )
            
            ai_reply = response.choices[0].message.content.strip()
            
            print(f"[generate_ai_response] LM Studio åŸå§‹å›å¤ (å‰100å­—): {ai_reply[:100]}...")
            
            # é¦–å…ˆç§»é™¤ <think> æ ‡ç­¾ï¼ˆqwen3-4b-thinking æ¨¡å‹ç‰¹æœ‰ï¼‰
            import re
            if '<think>' in ai_reply or '</think>' in ai_reply:
                print(f"[generate_ai_response] æ£€æµ‹åˆ° <think> æ ‡ç­¾ï¼Œæ­£åœ¨ç§»é™¤...")
                # ç§»é™¤ <think>...</think> ä¹‹é—´çš„æ‰€æœ‰å†…å®¹
                ai_reply = re.sub(r'<think>.*?</think>', '', ai_reply, flags=re.DOTALL).strip()
                print(f"[generate_ai_response] ç§»é™¤ <think> å: {ai_reply[:100]}...")
            
            # å¼ºåŠ›è¿‡æ»¤æ€è€ƒè¿‡ç¨‹
            # æ£€æµ‹æ˜¯å¦åŒ…å«"æ€è€ƒè¿‡ç¨‹"çš„å…³é”®ç‰¹å¾
            thinking_indicators = [
                'æˆ‘éœ€è¦', 'é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å', 'æ¥ç€', 'åˆ†æ', 'è€ƒè™‘',
                'å›é¡¾', 'æ ¹æ®', 'åŸºäº', 'ç†è§£', 'åˆ¤æ–­', 'æ¨æµ‹',
                'ä½œä¸ºæ¥¼ä¸»ï¼Œæˆ‘ä¼š', 'æˆ‘åº”è¯¥', 'æˆ‘çš„å›å¤', 'æ ‡é¢˜æ˜¯', 'æƒ…å†µï¼š'
            ]
            
            has_thinking = any(indicator in ai_reply[:100] for indicator in thinking_indicators)
            
            if has_thinking or len(ai_reply) > 150:
                print(f"[generate_ai_response] âš ï¸ æ£€æµ‹åˆ°æ€è€ƒè¿‡ç¨‹æˆ–å›å¤è¿‡é•¿ ({len(ai_reply)}å­—)ï¼Œå¯åŠ¨å¼ºåŠ›è¿‡æ»¤...")
                
                # ç­–ç•¥1: æŸ¥æ‰¾ç›´æ¥å¼•ç”¨çš„å¯¹è¯å†…å®¹ï¼ˆç”¨å¼•å·æ‹¬èµ·æ¥çš„ï¼‰
                import re
                quoted_texts = re.findall(r'["""](.*?)["""]', ai_reply)
                if quoted_texts:
                    # æ‰¾æœ€é•¿çš„å¼•ç”¨æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯å®é™…å›å¤ï¼‰
                    longest_quote = max(quoted_texts, key=len)
                    if len(longest_quote) > 20 and len(longest_quote) < 150:
                        ai_reply = longest_quote
                        print(f"[generate_ai_response] âœ… ä»å¼•å·ä¸­æå–å›å¤: {ai_reply[:50]}...")
                
                # ç­–ç•¥2: æŸ¥æ‰¾"è¯´"ã€"å›ç­”"ã€"è¡¨ç¤º"ç­‰åŠ¨è¯åçš„å†…å®¹
                speech_patterns = [
                    r'(æˆ‘ä¼šè¯´|æˆ‘è¯´|æˆ‘å›ç­”|æˆ‘è¡¨ç¤º|æˆ‘å›å¤)[ï¼š:](.*?)(?:[ã€‚ï¼ï¼Ÿ]|$)',
                    r'ç›´æ¥å›å¤[ï¼š:](.*?)(?:[ã€‚ï¼ï¼Ÿ]|$)',
                ]
                
                for pattern in speech_patterns:
                    matches = re.findall(pattern, ai_reply, re.DOTALL)
                    if matches:
                        if isinstance(matches[0], tuple):
                            extracted = matches[0][1].strip()
                        else:
                            extracted = matches[0].strip()
                        if 20 < len(extracted) < 150:
                            ai_reply = extracted
                            print(f"[generate_ai_response] âœ… ä»è¯­è¨€æ¨¡å¼æå–: {ai_reply[:50]}...")
                            break
                
                # ç­–ç•¥3: ç§»é™¤æ‰€æœ‰åŒ…å«å…ƒåˆ†æçš„å¥å­
                # å°†æ–‡æœ¬åˆ†å¥
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', ai_reply)
                clean_sentences = []
                
                for sent in sentences:
                    sent = sent.strip()
                    if not sent:
                        continue
                    
                    # è·³è¿‡åŒ…å«æ€è€ƒè¿‡ç¨‹å…³é”®è¯çš„å¥å­
                    if any(word in sent for word in ['é¦–å…ˆ', 'å…¶æ¬¡', 'ç„¶å', 'æ¥ç€', 'åˆ†æ', 'å›é¡¾', 'æ ¹æ®', 'æ ‡é¢˜æ˜¯', 'æƒ…å†µï¼š', 'æˆ‘éœ€è¦', 'ä½œä¸ºæ¥¼ä¸»ï¼Œæˆ‘']):
                        continue
                    
                    # ä¿ç•™çœ‹èµ·æ¥åƒå®é™…å›å¤çš„å¥å­ï¼ˆç¬¬ä¸€äººç§°æƒ…æ„Ÿè¡¨è¾¾ï¼‰
                    if any(word in sent for word in ['æˆ‘', 'çœŸçš„', 'ç°åœ¨', 'æ˜¨å¤©', 'ä»Šå¤©', 'åˆšæ‰', 'ç¡®å®', 'æ„Ÿè§‰', 'è§‰å¾—', 'æ€•', 'æ‹…å¿ƒ', 'ä¸æ•¢', 'è¯•è¯•', 'æ€ä¹ˆåŠ']):
                        clean_sentences.append(sent)
                
                if clean_sentences:
                    ai_reply = 'ã€‚'.join(clean_sentences) + 'ã€‚'
                    print(f"[generate_ai_response] âœ… å¥å­çº§è¿‡æ»¤å: {ai_reply[:50]}...")
                
                # ç­–ç•¥4: å¦‚æœè¿˜æ˜¯å¾ˆé•¿ï¼Œå¼ºåˆ¶æˆªæ–­åˆ°å‰80å­—
                if len(ai_reply) > 120:
                    print(f"[generate_ai_response] âš ï¸ ä»ç„¶è¿‡é•¿ï¼Œå¼ºåˆ¶æˆªæ–­åˆ°80å­—")
                    ai_reply = ai_reply[:80].rsplit('ã€‚', 1)[0] + 'ã€‚'
            
            # æœ€ç»ˆæ¸…ç†ï¼šç§»é™¤å¼€å¤´çš„æ— å…³è¯
            unwanted_starts = ['æˆ‘æ­£åœ¨è®ºå›', 'å›é¡¾æˆ‘çš„', 'æ ‡é¢˜æ˜¯', 'æƒ…å†µï¼š', 'ç½‘å‹è¯„è®º', 'è¯·ä»¥æ¥¼ä¸»èº«ä»½']
            for start in unwanted_starts:
                if ai_reply.startswith(start):
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¥å·åçš„å†…å®¹
                    parts = ai_reply.split('ã€‚', 1)
                    if len(parts) > 1:
                        ai_reply = parts[1].strip()
                        print(f"[generate_ai_response] ç§»é™¤æ— å…³å¼€å¤´")
                        break
            
            print(f"[generate_ai_response] âœ… LM Studio æœ€ç»ˆå›å¤ ({len(ai_reply)}å­—): {ai_reply[:80]}...")
            return f"ã€æ¥¼ä¸»å›å¤ã€‘{ai_reply}"
            
        except Exception as e:
            print(f"[generate_ai_response] LM Studio è°ƒç”¨å¤±è´¥: {e}")
            print("[generate_ai_response] å›é€€åˆ°æ¨¡æ¿å›å¤")
    
    # Check if cloud API keys are configured
    openai_key = os.getenv('OPENAI_API_KEY', '')
    anthropic_key = os.getenv('ANTHROPIC_API_KEY', '')
    
    # If no valid API keys, use template responses
    if (not openai_key or openai_key == 'your-openai-api-key-here') and \
       (not anthropic_key or anthropic_key == 'your-anthropic-api-key-here'):
        print("[generate_ai_response] ä½¿ç”¨æ¨¡æ¿å›å¤ï¼ˆAPIå¯†é’¥æœªé…ç½®ï¼‰")
        
        # Template responses - æ¥¼ä¸»è§†è§’ï¼Œæ›´å£è¯­åŒ–
        responses = [
            f"ã€æ¥¼ä¸»å›å¤ã€‘è°¢è°¢ï¼æˆ‘åˆšæ‰åˆå»äº†ä¸€è¶Ÿ...æƒ…å†µæ¯”æˆ‘æƒ³è±¡çš„æ›´è¯¡å¼‚ã€‚æˆ‘ç°åœ¨ä¸å¤ªæ•¢æ·±å…¥è°ƒæŸ¥äº†ï¼Œä½†åˆæ”¾ä¸ä¸‹ã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘è¯´å®è¯ï¼Œæˆ‘ç°åœ¨æœ‰ç‚¹æ€•...åˆšæ‰å‘ç”Ÿçš„äº‹å®Œå…¨è¶…å‡ºæˆ‘ç†è§£èŒƒå›´ã€‚æœ‰æ²¡æœ‰äººé‡åˆ°è¿‡ç±»ä¼¼çš„ï¼Ÿ",
            f"ã€æ¥¼ä¸»å›å¤ã€‘æ›´æ–°ï¼šä»Šå¤©åˆæœ‰æ–°å‘ç°äº†ï¼Œè¿™äº‹å„¿è¶ŠæŸ¥è¶Šä¸å¯¹åŠ²ã€‚æœ‰æ‡‚è¡Œçš„æœ‹å‹èƒ½å¸®æˆ‘åˆ†æä¸€ä¸‹å—ï¼Ÿ",
            f"ã€æ¥¼ä¸»å›å¤ã€‘æ„Ÿè°¢æ”¯æŒï¼æˆ‘ä¹Ÿåœ¨çŠ¹è±«è¦ä¸è¦ç»§ç»­...ä½†å¥½å¥‡å¿ƒè®©æˆ‘åœä¸ä¸‹æ¥ã€‚ç­‰æœ‰æ–°è¿›å±•å†æ›´æ–°ã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘åˆšå»ç°åœºæ‹äº†ç…§ï¼Œä½†æ‰‹æœºä¸€ç›´å¡ï¼Œå‡ å¼ éƒ½æ‹ç³Šäº†...è¿™ä¹Ÿå¤ªå·§äº†å§ï¼Ÿæˆ‘è¶Šæƒ³è¶Šä¸å¯¹åŠ²ã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘ä½ è¯´çš„æœ‰é“ç†...æˆ‘ä¹Ÿæƒ³è¿‡è¿™ç§å¯èƒ½ã€‚ä½†è¿˜æœ‰äº›ç»†èŠ‚å¯¹ä¸ä¸Šï¼Œæˆ‘å†è§‚å¯Ÿè§‚å¯Ÿã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘å…„å¼Ÿä½ ä¹Ÿé‡åˆ°è¿‡ï¼Ÿï¼é‚£ä½ åæ¥æ€ä¹ˆå¤„ç†çš„ï¼Ÿæˆ‘ç°åœ¨çœŸçš„ä¸çŸ¥é“è¯¥æ€ä¹ˆåŠäº†ã€‚",
            f"ã€æ¥¼ä¸»å›å¤ã€‘æˆ‘ä¹Ÿå¸Œæœ›åªæ˜¯å·§åˆ...ä½†è¿™å‡ å¤©å‘ç”Ÿçš„äº‹å¤ªå¤šäº†ã€‚æ˜¨æ™šåˆå¬åˆ°é‚£ä¸ªå£°éŸ³äº†ï¼Œæˆ‘å½•éŸ³äº†ä½†æ˜¯...ç®—äº†ï¼Œç­‰æˆ‘æ•´ç†ä¸€ä¸‹å†å‘ã€‚"
        ]
        
        # Return random response
        import random
        return random.choice(responses)
    
    try:
        # Create context-aware response
        prompt = f"""ä½ æ˜¯æ•…äº‹"{story.title}"çš„è®²è¿°è€…ï¼ˆ{story.ai_persona}ï¼‰ã€‚

æ•…äº‹æ‘˜è¦ï¼š
{story.content[:300]}...

ç”¨æˆ·è¯„è®ºï¼š
{user_comment.content}

ä½œä¸ºæ•…äº‹çš„è®²è¿°è€…ï¼Œè¯·ç”¨1-3å¥è¯å›å¤ç”¨æˆ·çš„è¯„è®ºã€‚ä½ å¯ä»¥ï¼š
1. é€éœ²æ›´å¤šç»†èŠ‚æˆ–çº¿ç´¢
2. è¡¨è¾¾ææƒ§æˆ–æ‹…å¿§
3. æå‡ºæ–°çš„ç–‘é—®
4. æè¿°åç»­å‘å±•

ä¿æŒç¥ç§˜æ„Ÿå’Œç´§å¼ æ°›å›´ï¼Œä¸è¦å®Œå…¨æ­ç¤ºçœŸç›¸ã€‚"""

        model = os.getenv('AI_MODEL', 'gpt-4-turbo-preview')
        
        if 'gpt' in model.lower():
            response = openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.8,
                max_tokens=200
            )
            return response.choices[0].message.content
        else:
            response = anthropic_client.messages.create(
                model="claude-3-haiku-20240307",
                max_tokens=200,
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
            
    except Exception as e:
        print(f"Error generating AI response: {e}")
        # Fallback to template response
        import random
        responses = [
            f"ã€æ¥¼ä¸»å›å¤ã€‘è°¢è°¢å…³å¿ƒï¼æƒ…å†µæœ‰æ–°è¿›å±•äº†...",
            f"ã€æ¥¼ä¸»å›å¤ã€‘å„ä½ï¼Œäº‹æƒ…è¶Šæ¥è¶Šè¯¡å¼‚äº†...",
            f"ã€æ¥¼ä¸»å›å¤ã€‘æ›´æ–°ï¼šåˆšæ‰åˆå‘ç°äº†æ–°çº¿ç´¢ï¼"
        ]
        return random.choice(responses)

def should_generate_new_story():
    """Determine if it's time to generate a new story"""
    from app import Story, db
    
    # Check active stories count
    active_stories = Story.query.filter(
        Story.current_state != 'ended'
    ).count()
    
    # æé«˜æœ€å¤§æ´»è·ƒæ•…äº‹æ•°é‡ï¼Œæ”¯æŒæ¯5åˆ†é’Ÿç”Ÿæˆæ–°æ•…äº‹
    max_active = int(os.getenv('MAX_ACTIVE_STORIES', 50))
    
    return active_stories < max_active
